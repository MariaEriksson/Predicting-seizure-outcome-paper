{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost model\n",
    "\n",
    "This is an XGBoost model, which uses clinical variables identified through data-driven feature selection to predict seizure outcome (seizure-free versus not seizure-free).\n",
    "\n",
    "The dataframe has - prior to this - been processed in the following manner:\n",
    "\n",
    "1. Predictors have been selected\n",
    "2. Categorical predictors have been one-hot encoded\n",
    "3. Missing data have been dropped (in the case of continuous variables as well as the categorical variable seizure outcome) or labelled as missing (in the case of categorical variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import binom\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataframe\n",
    "\n",
    "df_sf = pd.read_csv('/home/maria/Desktop/PhD/Predicting-outcome-clinical-paper/Dataframes/Outputs/df_complete_cohort_grouped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe\n",
    "\n",
    "df_sf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Implement the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X (predictors) and y (outcome)\n",
    "\n",
    "X = df_sf[df_sf.columns[~df_sf.columns.str.contains('one_year_sf')]]\n",
    "y = df_sf.one_year_sf\n",
    "\n",
    "# Convert the columns to float \n",
    "\n",
    "X = X.astype(np.float)\n",
    "y = y.astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Test the model using stratified k fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stratified k fold\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataframes to arrays\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "\n",
    "model_xgboost_own = XGBClassifier(base_score = 0.5, booster = 'gbtree', gamma = 0,\n",
    "                    max_depth = 1, eval_metric = 'error', subsample = 0.9,\n",
    "                    objective = 'binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train and test using stratified k fold cross-validation\n",
    "\n",
    "scores=np.zeros(len(X))\n",
    "y_pred=np.zeros(len(y))\n",
    "y_pred_fold=[]\n",
    "lst_accu_stratified = []\n",
    "cm_holder=[]\n",
    "auc_store = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # split data in stratified fold\n",
    "    x_train_fold, x_test_fold = X[train_index], X[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    # train on the fold\n",
    "    model_xgboost_own.fit(x_train_fold, y_train_fold)\n",
    "    scores_fold = model_xgboost_own.predict_proba(x_test_fold)[:,1]\n",
    "    auc_fold = metrics.roc_auc_score(y_test_fold, scores_fold)\n",
    "    auc_store.append(auc_fold)\n",
    "    scores[test_index] = scores_fold\n",
    "    y_pred[test_index] = model_xgboost_own.predict(x_test_fold)\n",
    "    lst_accu_stratified.append(model_xgboost_own.score(x_test_fold, y_test_fold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted probability\n",
    "\n",
    "plt.figure(0)\n",
    "plt.hist(scores[y==0], label='Not seizure free', color='b', alpha=0.5)\n",
    "plt.hist(scores[y==1], label='Seizure free', color='k', alpha=0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "\n",
    "confusion_matrix = pd.crosstab(y, y_pred, rownames=['Actual'], colnames=['Predicted'])\n",
    "\n",
    "x_axis_labels = ['Not seizure-free', 'Seizure-free'] # labels for x-axis\n",
    "y_axis_labels = ['Not seizure-free', 'Seizure-free'] # labels for y-axis\n",
    "\n",
    "# Change font \n",
    "annot_kws={'fontsize':12, \n",
    "           'alpha':0.6, \n",
    "           'verticalalignment':'center'\n",
    "          }\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(confusion_matrix, \n",
    "            annot=True,\n",
    "            annot_kws= annot_kws,\n",
    "            fmt='g',\n",
    "            xticklabels = x_axis_labels, \n",
    "            yticklabels = y_axis_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract values from confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y, y_pred)\n",
    "TP = confusion_matrix[1, 1]\n",
    "TN = confusion_matrix[0, 0]\n",
    "FP = confusion_matrix[0, 1]\n",
    "FN = confusion_matrix[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confidence interval for model accuracy\n",
    "\n",
    "count=(y_pred==y).sum()\n",
    "nobs=len(y)\n",
    "ci_low, ci_high = proportion_confint(count,\n",
    "                                     nobs, alpha=0.05, method='normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine how well the model performs\n",
    "\n",
    "print('List of accuracies obtained from each fold:', \n",
    "      lst_accu_stratified)\n",
    "\n",
    "print('\\nMaximum accuracy that can be obtained from this model:',\n",
    "      round(max(lst_accu_stratified)*100), '%')\n",
    "\n",
    "print('Minimum accuracy that can be obtained from this model:',\n",
    "      round(min(lst_accu_stratified)*100), '%')\n",
    "\n",
    "print('\\nOverall model accuracy:',\n",
    "      round(np.mean(lst_accu_stratified)*100), '%')\n",
    "\n",
    "print('CI low:', \n",
    "      round((ci_low)*100), '%')\n",
    "print('CI high:', \n",
    "      round((ci_high)*100), '%')\n",
    "\n",
    "print('Standard deviation:', \n",
    "      round(np.std(lst_accu_stratified),2))\n",
    "\n",
    "print('\\nNull acccuracy:', \n",
    "      round(max(y.mean(), 1 - y.mean())*100), '%')\n",
    "\n",
    "print('\\nClassification error:', \n",
    "      round((1 - metrics.accuracy_score(y, y_pred))*100), '%')\n",
    "\n",
    "print('\\nModel sensitivity:', \n",
    "      round((TP / float(FN + TP))*100), '%')\n",
    "\n",
    "print('Model specificity:', \n",
    "      round((TN / (TN + FP))*100), '%')\n",
    "\n",
    "print('\\nModel precision:', \n",
    "      round((TP / float(TP + FP))*100),'%')\n",
    "\n",
    "print('Model false positive rate:', \n",
    "      round((FP / float(TN + FP))*100),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC curve and AUC value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "fpr, tpr, n_thresh = metrics.roc_curve(y, scores, pos_label = 1)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AUC value\n",
    "\n",
    "print(metrics.roc_auc_score(y, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show AUC values for each of the 10 folds\n",
    "\n",
    "auc_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save values for figure\n",
    "\n",
    "with open('scores_xgboost_own_github.npy', 'wb') as f:\n",
    "    np.save(f, fpr)\n",
    "    np.save(f, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Evaluate performance at different sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X (predictors) and y (outcome)\n",
    "\n",
    "X2 = df_sf[df_sf.columns[~df_sf.columns.str.contains('one_year_sf')]]\n",
    "y2 = df_sf.one_year_sf\n",
    "\n",
    "# Convert the columns to float \n",
    "X2 = X2.astype(np.float)\n",
    "y2 = y2.astype(np.float)\n",
    "\n",
    "# Convert to array\n",
    "X2 = np.array(X2)\n",
    "y2 = np.array(y2)\n",
    "\n",
    "random_n = random.randint(0,20)\n",
    "iterations = np.arange(0,10)\n",
    "accuracy_batches_iterations=[]\n",
    "number_iterations = []\n",
    "for iteration in iterations:\n",
    "   \n",
    "    accuracy_batches=[]\n",
    "    number = []\n",
    "    index = []\n",
    "    n_subs = np.linspace(20,700,38).astype(int)\n",
    "\n",
    "    for ni, nsubs in enumerate(n_subs):\n",
    "        index = np.random.choice(np.arange(len(X2)),nsubs, replace = False)\n",
    "        print('There is {} patients'.format(len(index)))\n",
    "        X2sub = np.array(X2)[index]\n",
    "        y2sub = np.array(y2)[index]\n",
    "\n",
    "        # number per batches \n",
    "        n_n_sf = len(np.where(y2sub==0)[0])\n",
    "        print('number not seizure-free: ' + str(n_n_sf))\n",
    "        n_sf = len(np.where(y2sub==1)[0])\n",
    "        print('number seizure-free: ' + str(n_sf))\n",
    "\n",
    "        # Classifier\n",
    "        model2 = XGBClassifier(base_score = 0.5, booster = 'gbtree', gamma = 0,\n",
    "                    max_depth = 1, eval_metric = 'error', subsample=0.9,\n",
    "                    objective = 'binary:logistic')\n",
    "\n",
    "        # Stratified k fold cross-validation\n",
    "        scores=np.zeros(len(X2sub))\n",
    "        y2_pred=np.zeros(len(y2sub))\n",
    "        y2_pred_fold=[]\n",
    "        lst_accu_stratified = []\n",
    "        cm_holder=[]\n",
    "        for train_index, test_index in skf.split(X2sub, y2sub):\n",
    "            # split data in stratified fold\n",
    "            x2_train_fold, x2_test_fold = X2sub[train_index], X2sub[test_index]\n",
    "            y2_train_fold, y2_test_fold = y2sub[train_index], y2sub[test_index]\n",
    "            # train on the fold\n",
    "            model2.fit(x2_train_fold, y2_train_fold)\n",
    "            scores[test_index] = model2.predict_proba(x2_test_fold)[:,1]\n",
    "            y2_pred[test_index] = model2.predict(x2_test_fold)\n",
    "            lst_accu_stratified.append(model2.score(x2_test_fold, y2_test_fold))\n",
    "\n",
    "        accuracy=((y2_pred==y2sub).sum())/len(y2sub)\n",
    "        accuracy_batches.append(accuracy)\n",
    "        number.append(len(y2sub))\n",
    "    accuracy_batches_iterations.append(accuracy_batches)\n",
    "    number_iterations.append(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For iteration in iterations: \n",
    "\n",
    "accuracy_batches_mean = np.array(accuracy_batches_iterations).mean(axis=0)\n",
    "accuracy_batches_std = np.array(accuracy_batches_iterations).std(axis=0)\n",
    "plt.plot(number_iterations[0], accuracy_batches_mean , color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For iteration in iterations: \n",
    "\n",
    "accuracy_batches_mean = np.array(accuracy_batches_iterations).mean(axis=0)\n",
    "\n",
    "x = number_iterations[0]\n",
    "\n",
    "y_mins=[]\n",
    "y_maxs=[]\n",
    "\n",
    "for i in range(0,len(x)):\n",
    "    y_min, y_max = binom.interval(alpha=0.95, n=x[i], p=accuracy_batches_mean[i])\n",
    "    y_mins.append(y_min/x[i])\n",
    "    y_maxs.append(y_max/x[i])\n",
    "\n",
    "plt.plot(number_iterations[0], accuracy_batches_mean , color='k')\n",
    "plt.plot(number_iterations[0], y_mins , color='gray')\n",
    "plt.plot(number_iterations[0], y_maxs , color='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional packages\n",
    "\n",
    "from scipy.stats import powerlaw\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement power law function\n",
    "\n",
    "y = accuracy_batches_mean\n",
    "\n",
    "def func_inverse_powerlaw(x, a, b, c):\n",
    "    return (1-a)-b*x**c\n",
    "\n",
    "target_func = func_inverse_powerlaw\n",
    "\n",
    "popt, pcov = curve_fit(target_func, x, y,  maxfev=2000, bounds=[[0,-np.inf,-1],[1,np.inf,0]])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x, target_func(x, *popt), '--', color='blue', label='Fitted inverse power power-function ')\n",
    "plt.plot(x, y, 'ro', label='Learning curve points')\n",
    "plt.fill_between(x,y_min, y_max,color = 'orange', alpha = 0.15)\n",
    "plt.legend()\n",
    "plt.xticks(size = 15)\n",
    "plt.yticks(size = 15)\n",
    "plt.ylabel('Accuracy', fontsize = 20)\n",
    "plt.xlabel('Sample size', fontsize = 20)\n",
    "plt.legend(loc = 4,prop = {'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "x_expand = np.arange(1,2000)\n",
    "y_est=target_func(x_expand, *popt)\n",
    "y_mins= np.zeros(len(x_expand))\n",
    "y_maxs=np.zeros(len(x_expand))\n",
    "for i,x_exp in enumerate(x_expand):\n",
    "    y_min, y_max = binom.interval(alpha=0.95, n=x_exp, p=y_est[i])\n",
    "    y_mins[i] = y_min/x_exp\n",
    "    y_maxs[i] = y_max/x_exp\n",
    "y_maxs*=100\n",
    "y_mins*=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance at larger sample sizes\n",
    "\n",
    "# Expand x array\n",
    "y = accuracy_batches_mean\n",
    "\n",
    "# Create boundaries\n",
    "popt_min, pcov_min = curve_fit(target_func, x, y_min,  maxfev=2000, bounds=[[0,-np.inf,-1],[1,np.inf,0]])\n",
    "popt_max, pcov_max = curve_fit(target_func, x, y_max,  maxfev=2000, bounds=[[0,-np.inf,-1],[1,np.inf,0]])\n",
    "bound_upper = target_func(x_expand, *popt_max)*100\n",
    "bound_lower = target_func(x_expand, *popt_min)*100\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(x, y*100, 'ro', color='blue', label='Learning curve on dataset points')\n",
    "plt.plot(x, target_func(x, *popt)*100, '-', color='blue', label='Fitted inverse power-function')\n",
    "plt.plot(x_expand, target_func(x_expand, *popt)*100, '--', color='orange', label='Prediction on expanded dataset')\n",
    "plt.fill_between(x_expand, y_mins, y_maxs,color = 'orange', alpha = 0.15)\n",
    "plt.ylim([40,100])\n",
    "plt.xticks(size = 15)\n",
    "plt.yticks(size = 15)\n",
    "plt.ylabel('Accuracy (%)', fontsize = 20)\n",
    "plt.xlabel('Sample size', fontsize = 20)\n",
    "plt.legend(loc = 4,prop = {'size': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save values for figure\n",
    "\n",
    "with open('larger_sample_xgboost_own_github.npy', 'wb') as f:\n",
    "    np.save(f, x)\n",
    "    np.save(f, y)\n",
    "    np.save(f, popt)\n",
    "    np.save(f, y_mins)\n",
    "    np.save(f, y_maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
